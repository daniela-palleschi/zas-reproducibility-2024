@article{bochynska_reproducible_2023,
  title = {Reproducible Research Practices and Transparency across Linguistics},
  author = {Bochynska, Agata and Keeble, Liam and Halfacre, Caitlin and Casillas, Joseph V. and Champagne, Irys-Amélie and Chen, Kaidi and Röthlisberger, Melanie and Buchanan, Erin M. and Roettger, Timo B.},
  date = {2023-11-09},
  journaltitle = {Glossa Psycholinguistics},
  shortjournal = {Glossa Psycholinguistics},
  volume = {2},
  number = {1},
  issn = {2767-0279},
  doi = {10.5070/G6011239},
  url = {https://escholarship.org/uc/item/6m62j7p6},
  urldate = {2024-04-27},
  abstract = {Scientific studies of language span across many disciplines and provide evidence for social,\&nbsp; cultural, cognitive, technological, and biomedical studies of human nature and behavior. As it becomes increasingly empirical and quantitative, linguistics has been facing challenges and limitations of the scientific practices that pose barriers to reproducibility and replicability. One of the\&nbsp; proposed solutions to the widely acknowledged reproducibility and replicability crisis has been the implementation of transparency practices,\&nbsp; e.g., open access publishing, preregistrations, sharing study materials, data, and analyses, performing study replications, and declaring conflicts of interest. Here, we have assessed the prevalence of these practices in 600 randomly sampled journal articles from linguistics across two time points. In line with similar studies in other disciplines, we found that 35\% of the articles were published open access and the rates of sharing materials, data, and protocols were below 10\%. None of the articles reported preregistrations, 1\% reported replications, and 10\% had conflict of interest statements. These rates have not increased noticeably between 2008/2009 and 2018/2019, pointing to remaining barriers and the slow adoption of open and reproducible research practices in linguistics. To facilitate adoption of these practices, we provide a range of recommendations and solutions for implementing transparency and improving reproducibility of research in linguistics.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/PGLCZKLM/Bochynska et al. - 2023 - Reproducible research practices and transparency a.pdf}
}

@article{cruwell_seven_2019,
  title = {Seven {{Easy Steps}} to {{Open Science}}: {{An Annotated Reading List}}},
  shorttitle = {Seven {{Easy Steps}} to {{Open Science}}},
  author = {Crüwell, Sophia and Van Doorn, Johnny and Etz, Alexander and Makel, Matthew C. and Moshontz, Hannah and Niebaum, Jesse C. and Orben, Amy and Parsons, Sam and Schulte-Mecklenbeck, Michael},
  date = {2019-10},
  journaltitle = {Zeitschrift für Psychologie},
  shortjournal = {Zeitschrift für Psychologie},
  volume = {227},
  number = {4},
  pages = {237--248},
  issn = {2190-8370, 2151-2604},
  doi = {10.1027/2151-2604/a000387},
  url = {https://econtent.hogrefe.com/doi/10.1027/2151-2604/a000387},
  urldate = {2024-04-15},
  abstract = {The open science movement is rapidly changing the scientific landscape. Because exact definitions are often lacking and reforms are constantly evolving, accessible guides to open science are needed. This paper provides an introduction to open science and related reforms in the form of an annotated reading list of seven peer-reviewed articles, following the format of Etz, Gronau, Dablander, Edelsbrunner, and Baribault (2018). Written for researchers and students – particularly in psychological science – it highlights and introduces seven topics: understanding open science; open access; open data, materials, and code; reproducible analyses; preregistration and registered reports; replication research; and teaching open science. For each topic, we provide a detailed summary of one particularly informative and actionable article and suggest several further resources. Supporting a broader understanding of open science issues, this overview should enable researchers to engage with, improve, and implement current open, transparent, reproducible, replicable, and cumulative scientific practices.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/RV5VZALC/Crüwell et al. - 2019 - Seven Easy Steps to Open Science An Annotated Rea.pdf}
}

@article{hardwicke_populating_2018,
  title = {Populating the {{Data Ark}}: {{An}} Attempt to Retrieve, Preserve, and Liberate Data from the Most Highly-Cited Psychology and Psychiatry Articles},
  shorttitle = {Populating the {{Data Ark}}},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  editor = {Wicherts, Jelte M.},
  date = {2018-08-02},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {13},
  number = {8},
  pages = {e0201856},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0201856},
  url = {https://dx.plos.org/10.1371/journal.pone.0201856},
  urldate = {2024-04-27},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/LG227N75/Hardwicke and Ioannidis - 2018 - Populating the Data Ark An attempt to retrieve, p.pdf}
}

@article{ioannidis_why_2005,
  title = {Why Most Published Research Findings Are False},
  author = {Ioannidis, John P.A.},
  date = {2005},
  journaltitle = {PLoS Med},
  volume = {2},
  number = {8},
  eprint = {16060722},
  eprinttype = {pmid},
  pages = {2--8},
  issn = {15491277},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  isbn = {9783319513584},
  file = {/Users/danielapalleschi/Zotero/storage/W7EEEEIX/Ioannidis - 2005 - Why most published research findings are false.pdf}
}

@article{kathawalla_easing_2021,
  title = {Easing {{Into Open Science}}: {{A Guide}} for {{Graduate Students}} and {{Their Advisors}}},
  shorttitle = {Easing {{Into Open Science}}},
  author = {Kathawalla, Ummul-Kiram and Silverstein, Priya and Syed, Moin},
  date = {2021-01-26},
  journaltitle = {Collabra: Psychology},
  volume = {7},
  number = {1},
  pages = {18684},
  issn = {2474-7394},
  doi = {10.1525/collabra.18684},
  url = {https://online.ucpress.edu/collabra/article/doi/10.1525/collabra.18684/115927/Easing-Into-Open-Science-A-Guide-for-Graduate},
  urldate = {2023-03-09},
  abstract = {This article provides a roadmap to assist graduate students and their advisors to engage in open science practices. We suggest eight open science practices that novice graduate students could begin adopting today. The topics we cover include journal clubs, project workflow, preprints, reproducible code, data sharing, transparent writing, preregistration, and registered reports. To address concerns about not knowing how to engage in open science practices, we provide a difficulty rating of each behavior (easy, medium, difficult), present them in order of suggested adoption, and follow the format of what, why, how, and worries. We give graduate students ideas on how to approach conversations with their advisors/collaborators, ideas on how to integrate open science practices within the graduate school framework, and specific resources on how to engage with each behavior. We emphasize that engaging in open science behaviors need not be an all or nothing approach, but rather graduate students can engage with any number of the behaviors outlined.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/3MM594QZ/Kathawalla et al. - 2021 - Easing Into Open Science A Guide for Graduate Stu.pdf}
}

@article{laurinavichyute_share_2022,
  title = {Share the Code, Not Just the Data: {{A}} Case Study of the Reproducibility of Articles Published in the {{Journal}} of {{Memory}} and {{Language}} under the Open Data Policy},
  author = {Laurinavichyute, Anna and Yadav, Himanshu and Vasishth, Shravan},
  date = {2022},
  journaltitle = {Journal of Memory and Language},
  volume = {125},
  pages = {12},
  abstract = {In 2019 the Journal of Memory and Language instituted an open data and code policy; this policy requires that, as a rule, code and data be released at the latest upon publication. How effective is this policy? We compared 59 papers published before, and 59 papers published after, the policy took effect. After the policy was in place, the rate of data sharing increased by more than 50\%. We further looked at whether papers published under the open data policy were reproducible, in the sense that the published results should be possible to regenerate given the data, and given the code, when code was provided. For 8 out of the 59 papers, data sets were inaccessible. The reproducibility rate ranged from 34\% to 56\%, depending on the reproducibility criteria. The strongest predictor of whether an attempt to reproduce would be successful is the presence of the analysis code: it increases the probability of reproducing reported results by almost 40\%. We propose two simple steps that can increase the reproducibility of published papers: share the analysis code, and attempt to reproduce one’s own analysis using only the shared materials.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/ISZ9VFQU/Laurinavichyute - 2022 - Share the code, not just the data A case study of.pdf}
}

@article{open_science_collaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  date = {2015-08-28},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  url = {https://www.science.org/doi/10.1126/science.aac4716},
  urldate = {2024-04-17},
  abstract = {Empirically analyzing empirical evidence                            One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts               et al.               describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.                                         Science               , this issue               10.1126/science.aac4716                        ,              A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.           ,                             INTRODUCTION               Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.                                         RATIONALE               There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.                                         RESULTS                                We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and                 P                 values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (                 M                 r                 = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (                 M                 r                 = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (                 P                 {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.                                                        CONCLUSION                                No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original                 P                 value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.                              Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know.                                                   Original study effect size versus replication effect size (correlation coefficients).                   Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.                                                                         ,              Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/KEP964XF/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{peng_reproducible_2011,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, Roger D.},
  date = {2011-12-02},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {334},
  number = {6060},
  pages = {1226--1227},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  url = {https://www.science.org/doi/10.1126/science.1213847},
  urldate = {2024-05-15},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  langid = {english},
  file = {/Users/danielapalleschi/Zotero/storage/J5W9AUF4/Peng - 2011 - Reproducible Research in Computational Science.pdf}
}
